{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.llms import OpenAI, AzureOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm an AI language model, so I don't have feelings, but I'm here to help you. How can I assist you today?\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(content=\"Hello, how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\'markdown\\' cell: \\'[\\'---\\', \\'# Google Drive\\', \\'\\', \\'Manual: https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/google_drive\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from langchain.document_loaders import GoogleDriveLoader\\', \\'from langchain.document_loaders import UnstructuredFileIOLoader\\', \\'from pathlib import Path\\']\\'\\n\\n \\'code\\' cell: \\'[\\'folder_id = \"1KKCZgDYRNqLD-CYotYSfqxaSBb3QPJfB\"\\', \\'loader = GoogleDriveLoader(\\', \\'    folder_id=folder_id,\\', \\'    file_loader_cls=UnstructuredFileIOLoader,\\', \\'    file_loader_kwargs={\"mode\": \"elements\"},\\', \\'    service_account_key=Path(\"/Users/davidkuchelmeister/Documents/projects_coding/llm/original-frame-289508-7b253fd94c34.json\"),\\', \\')\\']\\'\\n\\n \\'code\\' cell: \\'[\\'docs = loader.load()\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from langchain.document_loaders import GoogleDriveLoader\\', \\'from langchain.document_loaders import UnstructuredFileIOLoader\\', \\'\\', \\'file_id = \"1evZdMhMwWGfgXiKHXFXytvOCE4PwSZZBt3XSGSIh\"\\', \\'loader = GoogleDriveLoader(\\', \\'    file_ids=[file_id],\\', \\'    file_loader_cls=UnstructuredFileIOLoader,\\', \\'    file_loader_kwargs={\"mode\": \"elements\"},\\', \\'    service_account_key=Path(\"/Users/davidkuchelmeister/Documents/projects_coding/llm/original-frame-289508-7b253fd94c34.json\"),\\', \\')\\']\\'\\n, gives error \\'HttpError\\',with description \\'<HttpError 404 when requesting https://www.googleapis.com/drive/v3/files/1evZdMhMwWGfgXiKHXFXytvOCE4PwSZZBt3XSGSIh?supportsAllDrives=true&alt=json returned \"File not found: 1evZdMhMwWGfgXiKHXFXytvOCE4PwSZZBt3XSGSIh.\". Details: \"[{\\'message\\': \\'File not found: 1evZdMhMwWGfgXiKHXFXytvOCE4PwSZZBt3XSGSIh.\\', \\'domain\\': \\'global\\', \\'reason\\': \\'notFound\\', \\'location\\': \\'fileId\\', \\'locationType\\': \\'parameter\\'}]\">\\'\\n\\n \\'code\\' cell: \\'[]\\'\\n, gives error \\'HttpError\\',with description \\'<HttpError 404 when requesting https://www.googleapis.com/drive/v3/files/1evZdMhMwWGfgXiKHXFXytvOCE4PwSZZBt3XSGSIh-Rg?supportsAllDrives=true&alt=json returned \"File not found: 1evZdMhMwWGfgXiKHXFXytvOCE4PwSZZBt3XSGSIh-Rg.\". Details: \"[{\\'message\\': \\'File not found: 1evZdMhMwWGfgXiKHXFXytvOCE4PwSZZBt3XSGSIh-Rg.\\', \\'domain\\': \\'global\\', \\'reason\\': \\'notFound\\', \\'location\\': \\'fileId\\', \\'locationType\\': \\'parameter\\'}]\">\\'\\n\\n \\'code\\' cell: \\'[]\\'\\n\\n', metadata={'source': '/Users/davidkuchelmeister/Documents/projects_coding/llm/src/llm/google_drive.ipynb'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import NotebookLoader\n",
    "\n",
    "loader = NotebookLoader(\n",
    "    \"/Users/davidkuchelmeister/Documents/projects_coding/llm/src/llm/google_drive.ipynb\",\n",
    "    include_outputs=True,\n",
    "    max_output_length=20,\n",
    "    remove_newline=True,\n",
    ")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You should explore the charming Old Town, visit the stunning Promenade des Anglais, and indulge in delicious Mediterranean cuisine.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "        AIMessage(content=\"You should go to Nice, France\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for Gravitational Potential Energy (GPE) is GPE = mgh, where m is the mass of the object, g is the acceleration due to gravity, and h is the height of the object. As an example, if a 5 kg object is dropped from a height of 10 meters, the GPE is equal to 5 x 9.81 x 10 = 490.5 J (Joules).\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "Document(\n",
    "    page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "    metadata={\n",
    "        'my_document_id' : 234234,\n",
    "        'my_document_source' : \"The LangChain Papers\",\n",
    "        'my_document_create_time' : 1680013019\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is length 1536\n",
      "Here's a sample: [-0.00011466221621958539, -0.0031506523955613375, -0.0007831145194359124, -0.019504327327013016, -0.015125557780265808]...\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "text = \"Hi! It's time for the beach\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "-----------\n",
      "LLM Output: Visit the Colosseum, the Vatican, and the Trevi Fountain.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=.7)\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: driver\n",
      "Example Output: car\n",
      "\n",
      "Example Input: pilot\n",
      "Example Output: plane\n",
      "\n",
      "Input: student\n",
      "Output:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' classroom'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples, \n",
    "    \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(chunk_size=1), \n",
    "    \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS, \n",
    "    \n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")\n",
    "\n",
    "# Select a noun\n",
    "my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))\n",
    "\n",
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Output formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\"),\n",
    "    ResponseSchema(name=\"reasoning\", description=\"This is your response, what and why it was wrong in 1-2 sentences\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format_instructions='The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"bad_string\": string  // This a poorly formatted user input string\\n\\t\"good_string\": string  // This is your response, a reformatted response\\n\\t\"reasoning\": string  // This is your response, what and why it was wrong in 1-2 sentences\\n}\\n```'\n",
      "promptValue='\\nYou will be given a poorly formatted string from a user.\\nReformat it and make sure all the words are spelled correctly\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"bad_string\": string  // This a poorly formatted user input string\\n\\t\"good_string\": string  // This is your response, a reformatted response\\n\\t\"reasoning\": string  // This is your response, what and why it was wrong in 1-2 sentences\\n}\\n```\\n\\n% USER INPUT:\\nwelcom to califonya!\\n\\nYOUR RESPONSE:\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to califonya!',\n",
       " 'good_string': 'Welcome to California!',\n",
       " 'reasoning': \"The user misspelled 'welcome' and 'California' and added an extra 'y' to the end.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# See the prompt template you created for formatting (default for the structured output parser, except for the passed ResponseSchema)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (f\"{format_instructions=}\")\n",
    "\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(f\"{promptValue=}\")\n",
    "\n",
    "llm_output = llm(promptValue)\n",
    "llm_output\n",
    "\n",
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Split Document and embedd each chunck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('/dbfs/tmp/mbrunzel@positivethinking.tech/langchain_tutorial_data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002', deployment='ptc-embedding-ada-002', chunk_size=1)\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "retriever\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")\n",
    "\n",
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Record chat history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "chat = AzureChatOpenAI(\n",
    "    model_name='gpt-35-turbo',\n",
    "    deployment_name='ptc-chat-gpt-35',\n",
    "    temperature=0.001\n",
    ")\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"what is the capital of france?\")\n",
    "\n",
    "history.messages\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "ai_response\n",
    "\n",
    "history.add_ai_message(ai_response.content)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# create templates with input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model_name='text-davinci-003',\n",
    "    deployment_name='ptc-davinci-003',\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "location_chain.prompt\n",
    "\n",
    "location_chain.run({\"user_location\": \"Berlin\"})\n",
    "\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
    "\n",
    "# Individual chain run with output of first chain\n",
    "meal_chain.run('An example of a classic Roman dish would be Spaghetti Carbonara. It is a dish made with spaghetti pasta, cured pork, eggs, black pepper, and parmesan cheese.')\n",
    "\n",
    "overall_chain.chains[1]\n",
    "\n",
    "review = overall_chain.run(\"Zurich\")\n",
    "\n",
    "review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
